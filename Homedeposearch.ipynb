{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravi\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\ravi\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn import cross_validation\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from SpellCheck import spell_check\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Function to convert a document to a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def prodinfo_to_wordlist( prodinfo_text):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # 1. Remove non-letters\n",
    "    prodinfo_text = re.sub(\"[^a-zA-Z]\",\" \", prodinfo_text)\n",
    "    # 2. Convert words to lower case, split them and remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = \" \".join([word for word in spell_check(prodinfo_text.lower()).split() if word not in stops])\n",
    "    # 3. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A function to split a prodinfo into parsed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prodinfo_to_sentences( prodinfo, tokenizer):\n",
    "    # Function to split a prodinfo into parsed sentences. Returns a\n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(prodinfo.strip())\n",
    "    # raw_sentences = tokenizer.tokenize(prodinfo.decode('utf-8').strip())\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "    # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call prodinfo_to_wordlist to get a list of words\n",
    "            sentences.append( prodinfo_to_wordlist( raw_sentence ))\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to average all of the word vectors in a given paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given a set of prodinfos (each one a list of words), calculate\n",
    "# the average feature vector for each one and return a 2D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(prodinfos, model, num_features):\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    prodinfoFeatureVecs = np.zeros((len(prodinfos),num_features),dtype=\"float32\")\n",
    "    # Loop through the prodinfos\n",
    "    for prodinfo in prodinfos:\n",
    "       #\n",
    "       # Print a status message every 1000th prodinfo\n",
    "       if counter%100. == 0.:\n",
    "           print \"done %d of %d\" % (counter, len(prodinfos))\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       prodinfoFeatureVecs[counter] = makeFeatureVec(prodinfo, model, num_features)\n",
    "       #Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return prodinfoFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "RMSE = make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "def RMSE(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_est_func(params):\n",
    "    n_estimators, learning_rate, max_depth, subsample = params\n",
    "    n_estimators=int(n_estimators)\n",
    "    print params\n",
    "    clf = GradientBoostingRegressor(n_estimators= n_estimators, learning_rate=learning_rate, max_depth=max_depth, subsample=subsample)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    rmse = RMSE( y_test, y_pred )\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(trials):\n",
    "    space = (\n",
    "             hp.quniform('n_estimators', 100,200, 100),\n",
    "             hp.quniform('learning_rate', 0.1, 0.2, 0.1),\n",
    "             hp.quniform('max_depth', 5, 6, 1),\n",
    "             hp.quniform('subsample', 0.5, 1, 0.5)\n",
    "             )\n",
    "    best = fmin(run_est_func, space, algo=tpe.suggest, trials=trials, max_evals=10)\n",
    "    print best\n",
    "\n",
    "random.seed(2016)\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('Data/test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv('Data/product_descriptions.csv',encoding=\"ISO-8859-1\")\n",
    "df_attribute = pd.read_csv('Data/attributes.csv',encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet01</td>\n",
       "      <td>Versatile connector for various 90Â° connectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet02</td>\n",
       "      <td>Stronger than angled nailing or screw fastenin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet03</td>\n",
       "      <td>Help ensure joints are consistently straight a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet04</td>\n",
       "      <td>Dimensions: 3 in. x 3 in. x 1-1/2 in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet05</td>\n",
       "      <td>Made from 12-Gauge steel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid      name                                              value\n",
       "0     100001.0  Bullet01  Versatile connector for various 90Â° connectio...\n",
       "1     100001.0  Bullet02  Stronger than angled nailing or screw fastenin...\n",
       "2     100001.0  Bullet03  Help ensure joints are consistently straight a...\n",
       "3     100001.0  Bullet04              Dimensions: 3 in. x 3 in. x 1-1/2 in.\n",
       "4     100001.0  Bullet05                           Made from 12-Gauge steel"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attribute.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Galvanized Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>100003.0</td>\n",
       "      <td>Composite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>100007.0</td>\n",
       "      <td>Plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>100009.0</td>\n",
       "      <td>Medium Density Fiberboard (MDF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>100010.0</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_uid                            value\n",
       "8       100001.0                 Galvanized Steel\n",
       "67      100003.0                        Composite\n",
       "202     100007.0                          Plastic\n",
       "255     100009.0  Medium Density Fiberboard (MDF)\n",
       "282     100010.0                            Steel"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attribute[df_attribute.name == \"Material\"][[\"product_uid\", \"value\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240760, 8)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('Data/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('Data/test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv('Data/product_descriptions.csv',encoding=\"ISO-8859-1\")\n",
    "df_attribute = pd.read_csv('Data/attributes.csv',encoding=\"ISO-8859-1\")\n",
    "df_brand = df_attribute[df_attribute.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n",
    "df_material = df_attribute[df_attribute.name == \"Material\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"material\"})\n",
    "\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')\n",
    "df_material1 = df_material.drop_duplicates(['product_uid'])\n",
    "df_all = pd.merge(df_all, df_material1, how='left', on='product_uid')\n",
    "print df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>relevance</th>\n",
       "      <th>search_term</th>\n",
       "      <th>product_description</th>\n",
       "      <th>brand</th>\n",
       "      <th>material</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>100001</td>\n",
       "      <td>3.00</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>Simpson Strong-Tie</td>\n",
       "      <td>Galvanized Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>100001</td>\n",
       "      <td>2.50</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>Simpson Strong-Tie</td>\n",
       "      <td>Galvanized Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n",
       "      <td>100002</td>\n",
       "      <td>3.00</td>\n",
       "      <td>deck over</td>\n",
       "      <td>BEHR Premium Textured DECKOVER is an innovativ...</td>\n",
       "      <td>BEHR Premium Textured DeckOver</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>100005</td>\n",
       "      <td>2.33</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "      <td>Delta</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>100005</td>\n",
       "      <td>2.67</td>\n",
       "      <td>shower only faucet</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "      <td>Delta</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      product_title product_uid  \\\n",
       "0   2                  Simpson Strong-Tie 12-Gauge Angle      100001   \n",
       "1   3                  Simpson Strong-Tie 12-Gauge Angle      100001   \n",
       "2   9  BEHR Premium Textured DeckOver 1-gal. #SC-141 ...      100002   \n",
       "3  16  Delta Vero 1-Handle Shower Only Faucet Trim Ki...      100005   \n",
       "4  17  Delta Vero 1-Handle Shower Only Faucet Trim Ki...      100005   \n",
       "\n",
       "   relevance         search_term  \\\n",
       "0       3.00       angle bracket   \n",
       "1       2.50           l bracket   \n",
       "2       3.00           deck over   \n",
       "3       2.33    rain shower head   \n",
       "4       2.67  shower only faucet   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  Not only do angles make joints stronger, they ...   \n",
       "1  Not only do angles make joints stronger, they ...   \n",
       "2  BEHR Premium Textured DECKOVER is an innovativ...   \n",
       "3  Update your bathroom with the Delta Vero Singl...   \n",
       "4  Update your bathroom with the Delta Vero Singl...   \n",
       "\n",
       "                            brand          material  \n",
       "0              Simpson Strong-Tie  Galvanized Steel  \n",
       "1              Simpson Strong-Tie  Galvanized Steel  \n",
       "2  BEHR Premium Textured DeckOver               NaN  \n",
       "3                           Delta               NaN  \n",
       "4                           Delta               NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload (sys)\n",
    "sys.setdefaultencoding('ISO-8859-1')\n",
    "\n",
    "stop_w = ['for', 'xbi', 'and', 'in', 'th','on','sku','with','what','from','that','less','er','ing'] #'electr','paint','pipe','light','kitchen','wood','outdoor','door','bathroom'\n",
    "strNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9}\n",
    "\n",
    "print (\"String clean ups...\")\n",
    " \n",
    "df_all['brand'] = np.where(df_all['brand'].isin (['n a ','na',' na','nan']),'unbrand',df_all['brand'])\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "df_all[\"search_term\"] = df_all[\"search_term\"].map(lambda x: regex.sub(\"\", x))\n",
    "df_all[\"product_title\"] = df_all[\"product_title\"].map(lambda x: regex.sub(\"\", x))\n",
    "df_all[\"product_description\"] = df_all[\"product_description\"].map(lambda x: regex.sub(\"\", x))\n",
    "df_all['combined_info'] = df_all['search_term'] + \" \" + \\\n",
    "                                    df_all['product_title'] + \" \" + \\\n",
    "                                    df_all['product_description']\n",
    "\n",
    "df_all.to_csv('Data/df_all_vect.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['combined_info'] = df_all['search_term'] + \" \" + \\\n",
    "                                    df_all['product_title'] + \" \" + \\\n",
    "                                    df_all['product_description']\n",
    "\n",
    "df_all.to_csv('Data/df_all_vect.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    angle bracket Simpson StrongTie 12Gauge Angle ...\n",
       "1    l bracket Simpson StrongTie 12Gauge Angle Not ...\n",
       "2    deck over BEHR Premium Textured DeckOver 1gal ...\n",
       "3    rain shower head Delta Vero 1Handle Shower Onl...\n",
       "4    shower only faucet Delta Vero 1Handle Shower O...\n",
       "Name: combined_info, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.combined_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have everything now lets start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240760"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"combined_info\"].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "from gensim.models import Word2Vec\n",
    "print \"Parsing sentences from training set\"\n",
    "for prodinfo in df_all[\"combined_info\"]:\n",
    "    sentences += prodinfo_to_sentences(prodinfo, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183613153"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec\n",
    "# creates nice output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set values for various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "size = 300\n",
    "# Minimum word count threshold.\n",
    "min_count = 3\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "workers = multiprocessing.cpu_count()\n",
    "# Context window length.\n",
    "window = 20\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "sample=1e-3\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and train the model (this will take some time) \n",
    "uncomment line below to train the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Training Word2Vec model...\"\n",
    "import gensim.models.word2vec as w2v\n",
    "model = w2v.Word2Vec(size = size, sg = 1, min_count = min_count, workers = workers, window = window\n",
    "             , sample = sample, seed = seed, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.train(sentences, total_examples = model.corpus_count, epochs = model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name1 = \"300features_30minwords_20context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the W2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"300features_30minwords_20context1\")\n",
    "print 'Model loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress the word vectors into 2D space and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix = thrones2vc.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tsne model, can take a while depending upon the machine its run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[thrones2vc.wv.vocab[word].index])\n",
    "            for word in thrones2vc.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x38800f60>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom in to some interesting places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-6f63b0447836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_Vecs'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcleantxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'vec_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5b0dd6c9d763>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[0;34m(prodinfos, model, num_features)\u001b[0m\n\u001b[1;32m     11\u001b[0m            \u001b[1;32mprint\u001b[0m \u001b[1;34m\"done %d of %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprodinfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m        \u001b[1;31m# Call the function (defined above) that makes average feature vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m        \u001b[0mprodinfoFeatureVecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakeFeatureVec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprodinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m        \u001b[1;31m#Increment the counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m        \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "num_style_desc = df_all.shape[0]\n",
    "for col in ['search_term','product_title','product_description']:\n",
    "    cleantxt=[]\n",
    "    for i in xrange( 0, num_style_desc ):\n",
    "        if( (i+1)%10000== 0 ):\n",
    "            print \"vectors %d of %d\\n\" % ( i+1, num_style_desc )\n",
    "        cleantxt.append(prodinfo_to_wordlist(df_all[col][i]) )\n",
    "    print col\n",
    "    vec = col+'_Vecs'\n",
    "    num_features=300\n",
    "    vec = getAvgFeatureVecs( cleantxt, model, num_features )\n",
    "    print vec.shape\n",
    "    df_vec = pd.DataFrame(vec,columns = ['vec_'+col+'_'+str(k) for k in range(num_features)])\n",
    "    df_vec.to_csv('Data/'+col+'_word2vecs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('Data/df_all_vect.csv', encoding=\"ISO-8859-1\")\n",
    "srch_vec = pd.read_csv('Data/search_term_word2vecs.csv').fillna(0.0)\n",
    "pdt_ttl_vec = pd.read_csv('Data/product_title_word2vecs.csv').fillna(0.0)\n",
    "pdt_desc_vec = pd.read_csv('Data/product_description_word2vecs.csv').fillna(0.0)\n",
    "srch_vec = srch_vec.as_matrix(columns=[srch_vec.columns[:300]])\n",
    "pdt_ttl_vec = pdt_ttl_vec.as_matrix(columns=[pdt_ttl_vec.columns[:300]])\n",
    "pdt_desc_vec = pdt_desc_vec.as_matrix(columns=[pdt_desc_vec.columns[:300]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dst_srch_ttl1 = np.zeros(srch_vec.shape[0])\n",
    "for i in range(srch_vec.shape[0]):\n",
    "    d1 = srch_vec[i, :]\n",
    "    d2 = pdt_ttl_vec[i, :]\n",
    "    dst_srch_ttl1[i] = cosine_similarity(d1, d2)\n",
    "dst_srch_desc1 = np.zeros(srch_vec.shape[0])\n",
    "for i in range(srch_vec.shape[0]):\n",
    "    d1 = srch_vec[i, :]\n",
    "    d2 = pdt_desc_vec[i, :]\n",
    "    dst_srch_desc1[i] = cosine_similarity(d1, d2)\n",
    "dst_ttl_desc1 = np.zeros(srch_vec.shape[0])\n",
    "for i in range(srch_vec.shape[0]):\n",
    "    d1 = pdt_ttl_vec[i, :]\n",
    "    d2 = pdt_desc_vec[i, :]\n",
    "    dst_srch_desc1[i] = cosine_similarity(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=30, random_state=2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srch_vec = svd.fit_transform(srch_vec)\n",
    "pdt_ttl_vec = svd.fit_transform(pdt_ttl_vec)\n",
    "pdt_desc_vec = svd.fit_transform(pdt_desc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srch_vec = pd.DataFrame(srch_vec, columns=['srch_vec_' + str(i) for i in range(srch_vec.shape[1])])\n",
    "pdt_ttl_vec = pd.DataFrame(pdt_ttl_vec, columns=['ttl_vec_' + str(i) for i in range(pdt_ttl_vec.shape[1])])\n",
    "pdt_desc_vec = pd.DataFrame(pdt_desc_vec, columns=['desc_vec_' + str(i) for i in range(pdt_desc_vec.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id = list(df_all['id'])\n",
    "srch_vec['id'] = id\n",
    "pdt_ttl_vec['id'] = id\n",
    "pdt_desc_vec['id'] = id\n",
    "\n",
    "df_all = pd.merge(df_all, srch_vec, how='left', on='id')\n",
    "df_all = pd.merge(df_all, pdt_ttl_vec, how='left', on='id')\n",
    "df_all = pd.merge(df_all, pdt_desc_vec, how='left', on='id')\n",
    "\n",
    "df_all['dst_srch_ttl1'] = dst_srch_ttl1\n",
    "df_all['dst_srch_desc1'] = dst_srch_desc1\n",
    "df_all['dst_ttl_desc1'] = dst_ttl_desc1\n",
    "\n",
    "cols = list(df_all.select_dtypes(include=['object']).columns)\n",
    "\n",
    "df_all1 = df_all.drop(cols, 1)\n",
    "df_all1.to_csv('../../Data/df_all_new_feat3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "df_all = pd.read_csv('../../Data/df_all_new_feat3.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "df_val = df_all[df_all['relevance'].isnull()]\n",
    "df_train = df_all[~df_all['relevance'].isnull()]\n",
    "id_val = df_val['id']\n",
    "y_train = df_train['relevance'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train, y_train, test_size=0.3, random_state=1234)\n",
    "\n",
    "X_train = X_train.drop(['id', 'relevance'], axis=1)\n",
    "X_test = X_test.drop(['id', 'relevance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
